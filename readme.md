# Local RAG Energy Assistant (Streamlit + Ollama)

A lightweight **Retrieval-Augmented Generation (RAG)** system that runs entirely **locally**, using  
**[Ollama](https://ollama.ai)** for large language models and embeddings, and **Streamlit** for a friendly chat interface.

![Live energy assistant chatbox](images/LLL-RAG.png)
---

## Overview

This project demonstrates how to combine **local LLMs** with **semantic retrieval** for an energy-focused assistant.

The app runs a **router model** that decides whether a user question:
- needs **live numerical data from National Grid API** (â†’ `TOOL`),
- requires **contextual knowledge** (â†’ `RAG`),
- or is just **small talk** (â†’ `CHITCHAT`).

Depending on the route:
- **TOOL** â†’ extracts time expressions and fetches live or forecasted carbon-intensity values from the **National Grid API**.  
- **RAG** â†’ retrieves relevant chunks from **locally embedded Markdown documents**.  
  The user query is embedded with `nomic-embed-text`, compared (cosine similarity) with stored document vectors (`vectors.npy`),  
  and the top-k most similar chunks form the context for the final answer.
- **CHITCHAT** â†’ uses the chat model directly for conversational replies.

All models â€” chat and embedding â€” run **fully offline** through Ollama.

---

##  Architecture
User â†’ Streamlit UI
â”‚
â–¼
route_query() â”€â”€ llama3:instruct (zero-shot router)
â”‚
â”œâ”€â”€ TOOL     â†’ extract_when() â†’ National Grid API â†’ llm_answer()
â”œâ”€â”€ RAG      â†’ retrieve() â†’ embed query â†’ cosine search on vectors.npy
â”‚               â†’ nearest document chunks â†’ llm_answer()
â””â”€â”€ CHITCHAT â†’ direct chat

---

## Tech Stack

| Component | Description |
|------------|--------------|
| **Ollama** | Local LLM runtime (chat + embeddings) |
| **llama3:instruct / llama3.1** | Chat model for reasoning & answers |
| **nomic-embed-text** | Embedding model for document vectors |
| **Streamlit** | Browser-based chat interface |
| **Python** | Core orchestration logic |
| **NumPy / JSON** | Lightweight vector store (`vectors.npy` + `chunks.json`) |

---

## ðŸª´ Setup & Run

### 1. Pull models
```bash
ollama pull llama3.instruct
ollama pull nomic-embed-text
```

### 2. Install dependencies
```bash
python -m venv .venv
source .venv/bin/activate     # Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

### 3. Build the local document index
```bash
python -m app.build_index
```

This creates:
	â€¢	data/chunks.json â†’ text + metadata
	â€¢	data/vectors.npy â†’ embeddings (via nomic-embed-text)

### 4. Launch the app
Start Ollama in one terminal:
```bash
ollama serve
```
Then run Streamlit in another:
```bash
python -m streamlit run ui/streamlit_app.py
```


## Project Structure

â”œâ”€ streamlit_app.py           # Streamlit UI
â”œâ”€ app/
â”‚   â”œâ”€ router_llm.py          # zero-shot router (TOOL / RAG / CHITCHAT)
â”‚   â”œâ”€ orchestrate.py         # main logic & answer()
â”‚   â”œâ”€ retrieval.py           # embedding-based retriever
â”‚   â”œâ”€ build_index.py         # embed Markdown docs â†’ vectors.npy + chunks.json
â”‚   â””â”€ tools.py               # API calls (e.g., National Grid)
â”œâ”€ data/
â”‚   â”œâ”€ docs/                  # local knowledge base (.md)
â”‚   â”œâ”€ chunks.json
â”‚   â””â”€ vectors.npy
â””â”€ requirements.txt